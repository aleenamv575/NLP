{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "** Lemmatization always considers the context and converts the word to its meaningful root/dictionary(WordNet) form called Lemma.**"
      ],
      "metadata": {
        "id": "8s8MXQMHQ6xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM1XTIX1FGrJ",
        "outputId": "98bfb5e5-29be-43c0-b39e-258a7a4493e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlBS1NP0HyRr",
        "outputId": "a123634b-d115-41f9-a2dc-ab9f243b69f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a lexical database (a collection of words) that has been used by major search engines and IR research projects for many years. It offers lemmatization capabilities as well and is one of the earliest and most commonly used lemmatizers."
      ],
      "metadata": {
        "id": "972CVLkNRINC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Sebpu3QYH4cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "V0YBEW2aH6Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"portabilities\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0V_d_joH_WC",
        "outputId": "29ce7603-daa4-4199-c75d-ef35832f61e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent=\"\"\"Great project for its size. love the portabilities. Picture qualities is 440p, but for the kids it is loveliest. \"\"\""
      ],
      "metadata": {
        "id": "5BZ0cXhtIHVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = nltk.word_tokenize(example_sent)\n",
        "print(word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln3629UvISxl",
        "outputId": "ee8c593f-a67e-4c4a-d941-160b0a1ac319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'project', 'for', 'its', 'size', '.', 'love', 'the', 'portabilities', '.', 'Picture', 'qualities', 'is', '440p', ',', 'but', 'for', 'the', 'kids', 'it', 'is', 'loveliest', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print([lemmatizer.lemmatize(get_wordnet_pos(w)) for w in nltk.word_tokenize(example_sent)])\n",
        "for i in word_list:\n",
        "  print(lemmatizer.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNAUf4MmIXtQ",
        "outputId": "443fb704-3026-40e0-bad2-e077c828f174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great\n",
            "project\n",
            "for\n",
            "it\n",
            "size\n",
            ".\n",
            "love\n",
            "the\n",
            "portability\n",
            ".\n",
            "Picture\n",
            "quality\n",
            "is\n",
            "440p\n",
            ",\n",
            "but\n",
            "for\n",
            "the\n",
            "kid\n",
            "it\n",
            "is\n",
            "loveliest\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li=[lemmatizer.lemmatize(i) for i in word_list]"
      ],
      "metadata": {
        "id": "0rmyYsn3IeRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(li)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZLCV7_kIlLa",
        "outputId": "a514674f-1888-4f44-de80-bdeeae04f8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'project', 'for', 'it', 'size', '.', 'love', 'the', 'portability', '.', 'Picture', 'quality', 'is', '440p', ',', 'but', 'for', 'the', 'kid', 'it', 'is', 'loveliest', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize Single Word\n",
        "print(lemmatizer.lemmatize(\"bats\"))\n",
        "#> bat\n",
        "\n",
        "print(lemmatizer.lemmatize(\"are\"))\n",
        "#> are\n",
        "\n",
        "print(lemmatizer.lemmatize(\"feet\"))\n",
        "#> foot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x25VPgkVRptu",
        "outputId": "e6175193-dcaa-4175-aa20-96c2063c85ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bat\n",
            "are\n",
            "foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt') #punkt is used for tokenising sentences \n",
        "nltk.download('averaged_perceptron_tagger') #averaged_perceptron_tagger is used for tagging words with their parts of speech (POS). "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nvDqp8lQnj9",
        "outputId": "b7d432b3-5b52-455a-a158-1c8c526f76d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "8iUESu2mKkJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "text = \"I also once encountered bluetooth on the plain of Vidya. Alas his latencies ended him. This headsets was my victorious saving graces.Heard some melodies and symphonies.The quality is good\"\n",
        "tokens = word_tokenize(text)\n",
        "lemma_function = WordNetLemmatizer()\n",
        "for token, tag in pos_tag(tokens):\n",
        "\t\tlemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
        "\t\tprint(token, \"=>\", lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hImZnwmlKn8o",
        "outputId": "5ccf2c45-61bc-45fc-806b-0fe4bf387fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => I\n",
            "also => also\n",
            "once => once\n",
            "encountered => encounter\n",
            "bluetooth => bluetooth\n",
            "on => on\n",
            "the => the\n",
            "plain => plain\n",
            "of => of\n",
            "Vidya => Vidya\n",
            ". => .\n",
            "Alas => Alas\n",
            "his => his\n",
            "latencies => latency\n",
            "ended => end\n",
            "him => him\n",
            ". => .\n",
            "This => This\n",
            "headsets => headset\n",
            "was => be\n",
            "my => my\n",
            "victorious => victorious\n",
            "saving => save\n",
            "graces.Heard => graces.Heard\n",
            "some => some\n",
            "melodies => melody\n",
            "and => and\n",
            "symphonies.The => symphonies.The\n",
            "quality => quality\n",
            "is => be\n",
            "good => good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TextBlob is a python library used for processing textual data. It provides a simple API to access its methods and perform basic NLP tasks. # TextBlob.correct() method, we are able to get the correct sentence without any spelling mistakes."
      ],
      "metadata": {
        "id": "IYLMFyB0T1ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        " \n",
        "my_word = 'cats'\n",
        " \n",
        "# create a Word object\n",
        "w = Word(my_word)\n",
        " \n",
        "print(w.lemmatize())\n",
        "#> cat\n",
        " \n",
        "sentence = 'the bats had cracks with streipes and the accessory for the feet was torn. I used to trust this compny as it alays valued ttheir employes'\n",
        "\n",
        "s = TextBlob(sentence)\n",
        "lemmatized_sentence = \" \".join([w.lemmatize() for w in s.words])\n",
        "print(lemmatized_sentence)\n",
        "\n",
        "\n",
        "\n",
        "#> the bat saw the cat with stripe hanging upside down by their foot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1GPmdN2T4Me",
        "outputId": "bd0ea8f0-06d8-4c04-cd34-6fe6947226c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "the bat had crack with streipes and the accessory for the foot wa torn I used to trust this compny a it alays valued ttheir employes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfg = TextBlob(\"the bat had crack with streipes and the accessory for the foot wa torn I used to trust this compny a it alays valued ttheir employes\")\n",
        " \n",
        "# using TextBlob.correct() method\n",
        "gfg = gfg.correct()\n",
        " \n",
        "print(gfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JafBUo6zWyE7",
        "outputId": "342bb7f1-bdbe-451f-b756-baf55aed3298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bat had crack with stripes and the accessory for the foot wa torn I used to trust this company a it always valued their employed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #Spacy provides convenient tools for breaking down sentences into lists of words and then categorizing each word with a specific part of speech based on the context.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        " \n",
        "# Create a Doc object\n",
        "doc = nlp(u' The bats were not upto the mark.The accessory of the feet was worse.')\n",
        " \n",
        "# Create list of tokens from given string\n",
        "tokens = []\n",
        "for token in doc:\n",
        "    tokens.append(token)\n",
        " \n",
        "print(tokens)\n",
        "\n",
        " \n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        " \n",
        "print(lemmatized_sentence)\n",
        "#> -PRON- foot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD9BkgWpUJ8b",
        "outputId": "f64c9f72-aab3-47c8-9cf0-2e5a066b791e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ , The, bats, were, not, upto, the, mark, ., The, accessory, of, the, feet, was, worse, .]\n",
            "  the bat be not upto the mark . the accessory of the foot be bad .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Even Pro-nouns were detected. ( identified by -PRON-)\n",
        "#Even best was changed to good."
      ],
      "metadata": {
        "id": "D-mVjax-UcrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f0KHyhCUqEJ",
        "outputId": "6de7c2fc-1a2d-458c-cb39-308a6bd9ce9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.9.1)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 29.4 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.2-py3-none-any.whl (10 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.14.0)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.1-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.2.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.2-py3-none-any.whl (6.0 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.9.1-py3-none-any.whl (10 kB)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.1-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.1.0)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.2-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.9.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (2022.6.2)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.1.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=afe4c72badf5d1f69a77c1e33d6d96b4417d57862e8c66b2b4ca75ef2e24fb56\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp37-cp37m-linux_x86_64.whl size=100003 sha256=d4fc8ad73f53bc4bc6a7fe165d824ca356dcc57f9b3df4ffdd4707a2783decbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/2d/67/2cb3f82e435fc8e055cb2761a15a0812bf086068f6fb835462\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=521dae74b7d7332be83734cf0df05494484eadc5598330ed46d718fd0de518e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=14a5265a10ae4fde4de3b8367169bd5d49fcf2568c86a42f4aaa0bdb90a9b9a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, autocommand, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed autocommand-2.2.1 backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.8.0 cryptography-38.0.1 feedparser-6.0.10 jaraco.classes-3.2.2 jaraco.collections-3.5.2 jaraco.context-4.1.2 jaraco.functools-3.5.1 jaraco.text-3.9.1 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20220524 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.2 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qtb4_IouWRRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3rxszkBWaER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sN8E5PYhYnVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}