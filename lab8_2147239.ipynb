{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYLNnNecKuAX"
      },
      "outputs": [],
      "source": [
        "#Stemming usually operates on single word without knowledge of the context\n",
        "#Lemmatization usually considers words and the context of the word in the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is faster compared to lemmatization as it cuts the prefixes(pre-, extra-, in-, im-, ir-, etc.)  and suffixes(ed-, ing-, es-, -ity, -ty, -ship, -ness, etc.) without considering the context of the words. Due to its aggressiveness, there is a possibility that the outcome from the stemming algorithm may not be a valid word."
      ],
      "metadata": {
        "id": "jtgmWegKP9JP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHY??\n",
        "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used a corpus also to supply lemma which makes it slower than stemming. you furthermore might had to define a parts-of-speech to get the proper lemma."
      ],
      "metadata": {
        "id": "ZPMzBhsUeyRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "word_data = \"I also once encountered bluetooth on the plain of Vidya. Alas his latencies ended him. This headsets was my victorious saving graces.Heard some melodies and symphonies.\"\n",
        "# First Word tokenization\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "#Next find the roots of the word\n",
        "for w in nltk_tokens:\n",
        "       print( \"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
        "       print (\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spcc2DRhNcOd",
        "outputId": "2608f228-8da3-4a67-f13c-047e021a94e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: I  Stem: i\n",
            "Actual: I  Lemma: I\n",
            "Actual: also  Stem: also\n",
            "Actual: also  Lemma: also\n",
            "Actual: once  Stem: onc\n",
            "Actual: once  Lemma: once\n",
            "Actual: encountered  Stem: encount\n",
            "Actual: encountered  Lemma: encountered\n",
            "Actual: bluetooth  Stem: bluetooth\n",
            "Actual: bluetooth  Lemma: bluetooth\n",
            "Actual: on  Stem: on\n",
            "Actual: on  Lemma: on\n",
            "Actual: the  Stem: the\n",
            "Actual: the  Lemma: the\n",
            "Actual: plain  Stem: plain\n",
            "Actual: plain  Lemma: plain\n",
            "Actual: of  Stem: of\n",
            "Actual: of  Lemma: of\n",
            "Actual: Vidya  Stem: vidya\n",
            "Actual: Vidya  Lemma: Vidya\n",
            "Actual: .  Stem: .\n",
            "Actual: .  Lemma: .\n",
            "Actual: Alas  Stem: ala\n",
            "Actual: Alas  Lemma: Alas\n",
            "Actual: his  Stem: hi\n",
            "Actual: his  Lemma: his\n",
            "Actual: latencies  Stem: latenc\n",
            "Actual: latencies  Lemma: latency\n",
            "Actual: ended  Stem: end\n",
            "Actual: ended  Lemma: ended\n",
            "Actual: him  Stem: him\n",
            "Actual: him  Lemma: him\n",
            "Actual: .  Stem: .\n",
            "Actual: .  Lemma: .\n",
            "Actual: This  Stem: thi\n",
            "Actual: This  Lemma: This\n",
            "Actual: headsets  Stem: headset\n",
            "Actual: headsets  Lemma: headset\n",
            "Actual: was  Stem: wa\n",
            "Actual: was  Lemma: wa\n",
            "Actual: my  Stem: my\n",
            "Actual: my  Lemma: my\n",
            "Actual: victorious  Stem: victori\n",
            "Actual: victorious  Lemma: victorious\n",
            "Actual: saving  Stem: save\n",
            "Actual: saving  Lemma: saving\n",
            "Actual: graces.Heard  Stem: graces.heard\n",
            "Actual: graces.Heard  Lemma: graces.Heard\n",
            "Actual: some  Stem: some\n",
            "Actual: some  Lemma: some\n",
            "Actual: melodies  Stem: melodi\n",
            "Actual: melodies  Lemma: melody\n",
            "Actual: and  Stem: and\n",
            "Actual: and  Lemma: and\n",
            "Actual: symphonies  Stem: symphoni\n",
            "Actual: symphonies  Lemma: symphony\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING DOES NOT USE POS TAGS\n",
        "#LEMMATIZATION USES POS TAGS"
      ],
      "metadata": {
        "id": "F6Q79IjPM81b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Real Time example showing use of Wordnet Lemmatization and POS Tagging in Python"
      ],
      "metadata": {
        "id": "QxmwDiymNaYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5zWcwHLgF5",
        "outputId": "30f1b28e-7825-4137-a6b1-1a5a8430ef1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "LAEP9VQ8K6Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "text = \"I also once encountered bluetooth on the plain of Vidya. Alas his latencies ended him. This headsets was my victorious saving graces.Heard some melodies and symphonies.\"\n",
        "tokens = word_tokenize(text)\n",
        "lemma_function = WordNetLemmatizer()\n",
        "for token, tag in pos_tag(tokens):\n",
        "\t\tlemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
        "\t\tprint(token, \"=>\", lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UuuZwH7LQvs",
        "outputId": "9937a0fd-efd4-4ef5-d7c6-9e455b423f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => I\n",
            "also => also\n",
            "once => once\n",
            "encountered => encounter\n",
            "bluetooth => bluetooth\n",
            "on => on\n",
            "the => the\n",
            "plain => plain\n",
            "of => of\n",
            "Vidya => Vidya\n",
            ". => .\n",
            "Alas => Alas\n",
            "his => his\n",
            "latencies => latency\n",
            "ended => end\n",
            "him => him\n",
            ". => .\n",
            "This => This\n",
            "headsets => headset\n",
            "was => be\n",
            "my => my\n",
            "victorious => victorious\n",
            "saving => save\n",
            "graces.Heard => graces.Heard\n",
            "some => some\n",
            "melodies => melody\n",
            "and => and\n",
            "symphonies => symphony\n",
            ". => .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Llj0l0gTMcWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing to note about lemmatization is that it is harder to create a lemmatizer in a new language than it is a stemming algorithm because we require a lot more knowledge about structure of a language in lemmatizers."
      ],
      "metadata": {
        "id": "FXAtNt96eg3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion : If speed is concentrated then stemming should be used since lemmatizers scan a corpus which consumes time and processing.\n",
        "It depends on the problem you’re working on that decides if stemmers should be used or lemmatizers."
      ],
      "metadata": {
        "id": "Un_B_r2tfchF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BzFhvzmGfaUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}