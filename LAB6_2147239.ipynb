{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Stemming  is nothing but reducing the words to their root/base by removing suffixes. helping,helped-\"help\"\n",
        " **STEMMING- TEXT NORMALIZATION**"
      ],
      "metadata": {
        "id": "dZACNf7sD73B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming algorithms: Porter Stemmer,Lancaster stemmer, Snowball Stemmer"
      ],
      "metadata": {
        "id": "EUiuJ53FEj3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porters Stemmer- is mainly known for Data Mining and Information Retrieval.  Applications are limited to the English language only.\n",
        "The suffixes in the English language are made up of a combination of smaller and simpler suffixes. \n",
        "Known for its simplicity and speed. \n",
        "The advantage- it produces the best output from other stemmers and has less error rate.**"
      ],
      "metadata": {
        "id": "jiSk8q-6IEfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmUl74sCE1Lk",
        "outputId": "12ccdd63-940d-4289-d7f1-a65f728f9ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "j_LkLMZvFLGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem('leak'))\n",
        "print(ps.stem('leaking'))\n",
        "print(ps.stem('leaked'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INxVYarnFYMC",
        "outputId": "d0ba4a2c-2814-4104-f73f-9b75eaabc81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leak\n",
            "leak\n",
            "leak\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem('make'))\n",
        "print(ps.stem('maked')) \n",
        "print(ps.stem('making'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VwCJKeoFzts",
        "outputId": "9078a195-39e1-4bfb-ab0a-ffabbb23a1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make\n",
            "make\n",
            "make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem('agreed'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53enmlLQaYaX",
        "outputId": "16c4d925-c81e-477d-e99d-7535c4f13099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yuzalc32GBur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\" Revision was Certified fairly \""
      ],
      "metadata": {
        "id": "4Q5LKHj2K5b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordList =nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "ek28-3noLZ2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemWords = [ps.stem(word)for word in wordList]"
      ],
      "metadata": {
        "id": "22NRbZxyLppc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' '.join(stemWords)) #wa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5nuI6_0MLOQ",
        "outputId": "1593e3ac-b13d-4613-ac18-519224e98295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "revis wa certifi fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#snowball Stemmer - English Stemmer or Porter2 Stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "Z7clp2msM6j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "jqt7JxGwNgTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2=\"Revision was certified fairly  \""
      ],
      "metadata": {
        "id": "Fb3wRWMrbAPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordList2 =nltk.word_tokenize(sentence2)"
      ],
      "metadata": {
        "id": "IIGGdp1NbfdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemWords2 = [ss.stem(word)for word in wordList2]"
      ],
      "metadata": {
        "id": "1oA01mqzNk7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' '.join(stemWords2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3VLOsksN-aj",
        "outputId": "6b9ea9d4-ea3b-4091-d0e4-e94092304aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "revis was certifi fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lancasterStemmer"
      ],
      "metadata": {
        "id": "FnsI9CUuH49u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer is the most aggressive stemming algorithm. It has an edge over other stemming techniques because it offers us the functionality to add our own custom rules in this algorithm when we implement this using the NLTK package. This sometimes results in abrupt results.\n",
        "hugely trims the word**"
      ],
      "metadata": {
        "id": "oC37FiBsIpvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer"
      ],
      "metadata": {
        "id": "5bfSLyj8I4uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['merely','electricity','vaguely','singing','caring']\n",
        "Lanc = LancasterStemmer()"
      ],
      "metadata": {
        "id": "CKcPCPN2JLmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "    print(w, \" : \", Lanc.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZTYMePmJcEa",
        "outputId": "98977463-5026-4053-86de-5dec7d77417a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merely  :  mer\n",
            "electricity  :  elect\n",
            "vaguely  :  vagu\n",
            "singing  :  sing\n",
            "caring  :  car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemWords = [Lanc.stem(word)for word in wordList]\n",
        "print(' '.join(stemWords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjksPH8dOTib",
        "outputId": "f95fc9e9-f2bd-47fd-d169-3acc6e078f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "revid was cert fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: Snowball Stemmer cames out to be one of the best suited algorithm for my need."
      ],
      "metadata": {
        "id": "zMoPNwJ-O9Lu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjSr2ZoG_rQP"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import GermanStemmer #Step 1 - Import the German language Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "NNLU5hjbLm3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "libAolLpHjrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "german_st = GermanStemmer() #Step 2 - Store the german stemmer in a variable"
      ],
      "metadata": {
        "id": "KS2647tnGNdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_sample = [\"Schreiben\",\"geschrieben\"] #Step 3- take samples"
      ],
      "metadata": {
        "id": "5QmuXTASGTpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words = [german_st.stem(words) for words in token_sample] #en #n-suffixes\n",
        "print(\"Print the output after stemming:\",stem_words) #Step 4 - Apply stemming and print the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pyZp7cqHQVo",
        "outputId": "d145c322-9c87-4434-e735-fe1a2b0d203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print the output after stemming: ['schreib', 'geschrieb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SRI Arabic Stemmer\n",
        "Arabic Stemming without a root dictionary. \n",
        "\n",
        "The Information Science Research Institute’s (ISRI) Arabic stemmer shares many features with the Khoja stemmer. However, the main difference is that ISRI stemmer does not use root dictionary. Also, if a root is not found, ISRI stemmer returned normalized form, rather than returning the original unmodified word.\n",
        "\n",
        "Additional adjustments were made to improve the algorithm to avoid  the word ambiguities and changes the original root."
      ],
      "metadata": {
        "id": "iNoRC2_g76XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.stem.isri import ISRIStemmer"
      ],
      "metadata": {
        "id": "toQ6kiUi5vLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " st = ISRIStemmer()"
      ],
      "metadata": {
        "id": "N1Gk-UK87gln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.stem(u'اعلاميون')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g_Itl46l7maa",
        "outputId": "26522e01-f7e8-431d-9819-a2fb79b98fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'علم'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar_text = 'سافر المسافرون بالطائرة واستمتعوا كثيرا بالرحلة'"
      ],
      "metadata": {
        "id": "3ti1MKga7rZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[st.stem(token) for token in ar_text.split()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1A86pZ87tS5",
        "outputId": "581251c2-1535-4aa7-ef6d-d3369843df1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['سفر', 'سفر', 'طئر', 'متع', 'كثر', 'رحل']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from snowballstemmer import stemmer\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "ar_stemmer.stemWord(u\"فسميتموها\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Wj-qFvZa70iG",
        "outputId": "603e7591-2006-4ef9-a33f-257c5a6d7913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'سمي'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import load\n",
        "from nltk.stem.api import StemmerI"
      ],
      "metadata": {
        "id": "xW-K5fMT97rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n"
      ],
      "metadata": {
        "id": "W4nxc5IG_N6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('rslp') #portugese"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOvtwrM9_yw-",
        "outputId": "588bc03a-a325-4c8e-b619-0b561b1443cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = RSLPStemmer()"
      ],
      "metadata": {
        "id": "yzE9MueS_mF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "        ... Clarissa risca com giz no quadro-negro a paisagem que os alunos\n",
        "        ... devem copiar . Uma casinha de porta e janela , em cima duma\n",
        "        ... coxilha .'''"
      ],
      "metadata": {
        "id": "y-6ubZ3V_4zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in text.split():\n",
        " print(st.stem(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERbHrXLL_6vI",
        "outputId": "6b45ffa1-3ae2-4a92-8dfc-6960a31b8092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n",
            "clariss\n",
            "risc\n",
            "com\n",
            "giz\n",
            "no\n",
            "quadro-negr\n",
            "a\n",
            "pais\n",
            "que\n",
            "os\n",
            "alun\n",
            "...\n",
            "dev\n",
            "copi\n",
            ".\n",
            "uma\n",
            "cas\n",
            "de\n",
            "port\n",
            "e\n",
            "janel\n",
            ",\n",
            "em\n",
            "cim\n",
            "dum\n",
            "...\n",
            "coxilh\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Languages supported by SnowballStemmer\n",
        "print (SnowballStemmer.languages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nQ0FmZuC12N",
        "outputId": "632da1db-4a6e-4616-8010-7bfb715c163a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer_spanish = SnowballStemmer('spanish')\n",
        "\n",
        "print (stemmer_spanish.stem('trabajando')) # output: trabaj\n",
        "print (stemmer_spanish.stem('trabajos')) # output: trabaj\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ad_2ZaBADs2",
        "outputId": "8df59810-77a7-43ac-ff8c-73d6629c33c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trabaj\n",
            "trabaj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word=\"Mi chiamo Francesca e lavoro come segretaria nella scuola elementare della mia città. Ogni mattina, mi sveglio alle 7 e preparo la colazione per tutta la famiglia.\""
      ],
      "metadata": {
        "id": "oRjOXGlxPC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import ItalianStemmer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "stemmer = ItalianStemmer()\n",
        "inflation=0\n",
        "stemmer_re=RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "for i in sample_word.split():\n",
        "  print(i,\"-->\",stemmer.stem(i),\"-->\",stemmer_re.stem(i))\n",
        "  if stemmer.stem(i)!=stemmer_re.stem(i):\n",
        "    inflation+=1\n",
        "print(\"inflation\",inflation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7s91arAPawB",
        "outputId": "df9f3934-450a-4c06-9ca1-5740e02b9ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mi --> mi --> Mi\n",
            "chiamo --> chiam --> chiamo\n",
            "Francesca --> francesc --> Francesca\n",
            "e --> e --> e\n",
            "lavoro --> lavor --> lavoro\n",
            "come --> com --> com\n",
            "segretaria --> segretar --> segretaria\n",
            "nella --> nell --> nella\n",
            "scuola --> scuol --> scuola\n",
            "elementare --> element --> elementar\n",
            "della --> dell --> della\n",
            "mia --> mia --> mia\n",
            "città. --> città. --> città.\n",
            "Ogni --> ogni --> Ogni\n",
            "mattina, --> mattina, --> mattina,\n",
            "mi --> mi --> mi\n",
            "sveglio --> svegl --> sveglio\n",
            "alle --> alle --> all\n",
            "7 --> 7 --> 7\n",
            "e --> e --> e\n",
            "preparo --> prepar --> preparo\n",
            "la --> la --> la\n",
            "colazione --> colazion --> colazion\n",
            "per --> per --> per\n",
            "tutta --> tutt --> tutta\n",
            "la --> la --> la\n",
            "famiglia. --> famiglia. --> famiglia.\n",
            "inflation 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word=\"Je m'appelle Camille et quand j'étais enfant, j'avais des rêves plein la tête. Je voulais être professeur et enseigner les mathématiques. Puis, je voulais devenir docteur pour soigner les malades. Pour moi, les médecins et les infirmières sont des héros sans costume.\""
      ],
      "metadata": {
        "id": "R5JDkFhjPgVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import FrenchStemmer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "stemmer = FrenchStemmer()\n",
        "inflation=0\n",
        "stemmer_re=RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "for i in sample_word.split():\n",
        "  print(i,\"-->\",stemmer.stem(i),\"-->\",stemmer_re.stem(i))\n",
        "  if stemmer.stem(i)!=stemmer_re.stem(i):\n",
        "    inflation+=1\n",
        "print(\"inflation\",inflation)\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FekKcEAPD5kO",
        "outputId": "341b1091-41db-466c-e687-9ec395c78ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je --> je --> Je\n",
            "m'appelle --> m'appel --> m'appell\n",
            "Camille --> camill --> Camill\n",
            "et --> et --> et\n",
            "quand --> quand --> quand\n",
            "j'étais --> j'et --> j'étai\n",
            "enfant, --> enfant, --> enfant,\n",
            "j'avais --> j'av --> j'avai\n",
            "des --> de --> des\n",
            "rêves --> rêv --> rêve\n",
            "plein --> plein --> plein\n",
            "la --> la --> la\n",
            "tête. --> tête. --> tête.\n",
            "Je --> je --> Je\n",
            "voulais --> voul --> voulai\n",
            "être --> être --> êtr\n",
            "professeur --> professeur --> professeur\n",
            "et --> et --> et\n",
            "enseigner --> enseign --> enseigner\n",
            "les --> le --> les\n",
            "mathématiques. --> mathématiques. --> mathématiques.\n",
            "Puis, --> puis, --> Puis,\n",
            "je --> je --> je\n",
            "voulais --> voul --> voulai\n",
            "devenir --> deven --> devenir\n",
            "docteur --> docteur --> docteur\n",
            "pour --> pour --> pour\n",
            "soigner --> soign --> soigner\n",
            "les --> le --> les\n",
            "malades. --> malades. --> malades.\n",
            "Pour --> pour --> Pour\n",
            "moi, --> moi, --> moi,\n",
            "les --> le --> les\n",
            "médecins --> médecin --> médecin\n",
            "et --> et --> et\n",
            "les --> le --> les\n",
            "infirmières --> infirmi --> infirmière\n",
            "sont --> sont --> sont\n",
            "des --> de --> des\n",
            "héros --> héros --> héro\n",
            "sans --> san --> san\n",
            "costume. --> costume. --> costume.\n",
            "inflation 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2cGFkg3PLjk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}